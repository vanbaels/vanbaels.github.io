[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sven Van Bael",
    "section": "",
    "text": "Mail\n  \n  \n    \n     CV\n  \n  \n    \n     Scholar\n  \n  \n    \n     ORCID\n  \n\n  \n  \nHi!\nI’m a former (post)doctoral researcher at KU Leuven/University of Antwerp with a strong interest in data science and data visualization. During my academic career, I have gained a lot of experience with critically analyzing large data sets and applying statistical methods to turn raw data into meaningful conclusions.\nOn this website I have collected some of my work with R, RStudio and Quarto. This ranges from personal projects based on my own interests and curiosity, to some of my academic work, where I used R to answer some of my research questions.\nWant to know more about me? Check the left pane of this page. You can contact me via mail, or explore my CV and list of academic publications."
  },
  {
    "objectID": "Personal_projects/posts/VetoPoll/VetoPoll.html",
    "href": "Personal_projects/posts/VetoPoll/VetoPoll.html",
    "title": "Some observations of the 2024 Veto federal election poll",
    "section": "",
    "text": "In the lead-up to the 2024 federal elections in Belgium, the student newspaper Veto conducted a survey among the students of its alma mater, the KU Leuven. On their website, Veto shows the results university-wide (Nollet et al., 2024a), and split up per faculty (Nollet et al., 2024b). For the university-wide results, the plot provides a breakdown of the votes per faculty for each of the political parties. At the time, this article was heavily discussed and debated in the media, prompting reactions from all sides from the political spectrum. Having a closer look at it, I did spot some interesting oddities that eventually sparked my interest for a more in-depth look.\nA first observation in the university-wide plot is that the number of votes exceeds the total number of participants of the faculty. For example, in the faculty of Economics and Business, 137 students have voted for N-VA (Figure 1A), while the faculty plot shows that there are only a total of 50 participants, of which 27 (54%) voted for N-VA (Figure 1B).\n\n\n\n\n\n\nFigure 1: Veto survey results for the whole university (A) versus the faculty of Economics and Business (B). The discrepancy in the number of votes for N-VA is due to a weight correction that has been applied for the university-wide plot.\n\n\n\nSo how did these 27 students in the faculty of Economics become 137 students university-wide? Luckily, the authors of the article did provide a short description of their methods for analyzing the data:\n\n[…] The results from the students were weighted according to the relative size of the participating faculties […]\nVeto\n\nThis reasoning is sound, as differences in survey participants between faculties can change their impact on the university-wide results. For example, two faculties can differ substantially in student numbers, but still have a comparable number of survey participants. As a result, both faculties will have equal contributions in the overall survey results. To correct for this, Veto has used the number of students of each faculty to calculate their relative sizes within the university, and then re-scaled the total number of participants at each faculty accordingly. It should be noted that such a correction process is only valid when you are working with data that is representative for the whole faculty, otherwise you will be extrapolating unreliable data.\nLet’s do a quick check for the earlier mentioned faculty of Economics and Business, where 50 students participated in the survey. Going by the student numbers per faculty (KU Leuven, 2024), Economics and Business had a total of 10443 students in 2022-2023 (the most recent academic year with complete data). This means that the voting preferences of 50 students - not even 0.5% of the total faculty - has been extrapolated to represent that of 10443 students! To make matters worse: the faculty of Economics is one of the largest at KU Leuven, representing 16% of the total student body. Expecting that this was just an exception, and that other participant numbers would be more representative, I did the same check for Medicine, another large faculty with 11103 students. Here, 91 students have participated in the survey, a meager 0.8% of the entire faculty.\nAs a consequence of the low participation and size of both faculties, the weight correction has extrapolated the voting preferences of only 141 students to represent those of 21,546 students, or ~33% of the university. This observation prompted me to have a look at what the data looked like before the correction was applied. Luckily, Veto did a very good job in creating informative plots, making it possible to reverse engineer the raw data from the faculty and university-wide plots."
  },
  {
    "objectID": "Personal_projects/posts/VetoPoll/VetoPoll.html#introduction",
    "href": "Personal_projects/posts/VetoPoll/VetoPoll.html#introduction",
    "title": "Some observations of the 2024 Veto federal election poll",
    "section": "",
    "text": "In the lead-up to the 2024 federal elections in Belgium, the student newspaper Veto conducted a survey among the students of its alma mater, the KU Leuven. On their website, Veto shows the results university-wide (Nollet et al., 2024a), and split up per faculty (Nollet et al., 2024b). For the university-wide results, the plot provides a breakdown of the votes per faculty for each of the political parties. At the time, this article was heavily discussed and debated in the media, prompting reactions from all sides from the political spectrum. Having a closer look at it, I did spot some interesting oddities that eventually sparked my interest for a more in-depth look.\nA first observation in the university-wide plot is that the number of votes exceeds the total number of participants of the faculty. For example, in the faculty of Economics and Business, 137 students have voted for N-VA (Figure 1A), while the faculty plot shows that there are only a total of 50 participants, of which 27 (54%) voted for N-VA (Figure 1B).\n\n\n\n\n\n\nFigure 1: Veto survey results for the whole university (A) versus the faculty of Economics and Business (B). The discrepancy in the number of votes for N-VA is due to a weight correction that has been applied for the university-wide plot.\n\n\n\nSo how did these 27 students in the faculty of Economics become 137 students university-wide? Luckily, the authors of the article did provide a short description of their methods for analyzing the data:\n\n[…] The results from the students were weighted according to the relative size of the participating faculties […]\nVeto\n\nThis reasoning is sound, as differences in survey participants between faculties can change their impact on the university-wide results. For example, two faculties can differ substantially in student numbers, but still have a comparable number of survey participants. As a result, both faculties will have equal contributions in the overall survey results. To correct for this, Veto has used the number of students of each faculty to calculate their relative sizes within the university, and then re-scaled the total number of participants at each faculty accordingly. It should be noted that such a correction process is only valid when you are working with data that is representative for the whole faculty, otherwise you will be extrapolating unreliable data.\nLet’s do a quick check for the earlier mentioned faculty of Economics and Business, where 50 students participated in the survey. Going by the student numbers per faculty (KU Leuven, 2024), Economics and Business had a total of 10443 students in 2022-2023 (the most recent academic year with complete data). This means that the voting preferences of 50 students - not even 0.5% of the total faculty - has been extrapolated to represent that of 10443 students! To make matters worse: the faculty of Economics is one of the largest at KU Leuven, representing 16% of the total student body. Expecting that this was just an exception, and that other participant numbers would be more representative, I did the same check for Medicine, another large faculty with 11103 students. Here, 91 students have participated in the survey, a meager 0.8% of the entire faculty.\nAs a consequence of the low participation and size of both faculties, the weight correction has extrapolated the voting preferences of only 141 students to represent those of 21,546 students, or ~33% of the university. This observation prompted me to have a look at what the data looked like before the correction was applied. Luckily, Veto did a very good job in creating informative plots, making it possible to reverse engineer the raw data from the faculty and university-wide plots."
  },
  {
    "objectID": "Personal_projects/posts/VetoPoll/VetoPoll.html#packages-used",
    "href": "Personal_projects/posts/VetoPoll/VetoPoll.html#packages-used",
    "title": "Some observations of the 2024 Veto federal election poll",
    "section": "2 Packages used",
    "text": "2 Packages used\n\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(scales)\nlibrary(ggalluvial)\nlibrary(ggrepel)"
  },
  {
    "objectID": "Personal_projects/posts/VetoPoll/VetoPoll.html#exploring-the-data",
    "href": "Personal_projects/posts/VetoPoll/VetoPoll.html#exploring-the-data",
    "title": "Some observations of the 2024 Veto federal election poll",
    "section": "3 Exploring the data",
    "text": "3 Exploring the data\nAll code that is used to generate each figure and table is available under “Show code”.\n\n3.1 Extracting the raw data from the plots was successful, with some minor missing data\nTo get the raw voting data, I went manually through the 16 faculty plots and recalculated the absolute votes per party from the percentage they scored and the total number of participants in each faculty. From the university-wide plot, I extracted the weight-corrected votes and added the column Correction to the data frame, containing the indicators absolute for the raw votes and weighted for the weight-corrected votes. The complete data set is available as Veto_survey_votes.csv. Next, I collected the faculty student numbers for the academic year 2022-2023 from the KU Leuven website (KU Leuven, 2024), and saved it as KUL_student_numbers.csv. Both data sets were combined, and the result is shown in Table 1.\n\n\nShow code\n#Read the .csv file containing the student numbers.\ndf_student_numbers &lt;- read_csv(\"KUL_student_numbers.csv\")\n\n#Read the .csv file containing the raw and weighted votes, and join with the student numbers data frame\ndf_votes &lt;- read_csv(\"Veto_survey_votes.csv\") %&gt;%\n  group_by(Faculty) %&gt;%\n  select(-`Participants - absolute`) %&gt;%\n  pivot_longer(cols = c(\"Votes - absolute\", \"Votes - weighted\"), names_to = \"Correction\", values_to = \"Votes\", names_prefix = \"Votes - \") %&gt;%\n  group_by(Faculty, Correction) %&gt;%\n  mutate(\"Participants\" = sum(Votes)) %&gt;%\n  full_join(., df_student_numbers, by = c(\"Faculty\", \"Abbreviation\")) %&gt;%\n  arrange(`Students faculty` %&gt;% desc())\n\n#Change Faculty, Abbreviation, Correction and Party to type factor\ndf_votes$Faculty &lt;- factor(df_votes$Faculty)\ndf_votes$Abbreviation &lt;- factor(df_votes$Abbreviation)\ndf_votes$Correction &lt;- factor(df_votes$Correction, levels = c(\"weighted\", \"absolute\"))\ndf_votes$Party &lt;- factor(df_votes$Party, levels = c(\"N-VA\", \"Groen\", \"Vooruit\", \"Open VLD\", \"CD&V\", \"PVDA\", \"Vlaams Belang\", \"Voor U\", \"I'm not going to vote\", \"L'Unie\", \"I vote blank\", \"Partij Blanco\"))\n\ndatatable(df_votes, filter = \"top\", rownames = F)\n\n\n\n\nTable 1: Vote and faculty statistics of all 16 KU Leuven faculties that participated in the survey. Data was extracted from the Veto article, and KU Leuven website.\n\n\n\n\n\n\n\n\n\n\n\nWhen summarizing this table for the entire university (Table 2), the total number of students that participated in the survey is 1801. For the weight correction, this number drops to 1735. Annoyingly, this value of weight-corrected votes does not correspond with what is indicated on the university wide plot in the Veto article (also shown in Figure 1), which states that the total number of weight corrected votes equals 1728.\n\n\nShow code\n#Summing the absolute and weight-corrected votes to get the total number of particpants for each.\ndf_votes %&gt;%\n  group_by(Correction) %&gt;%\n  summarize(\"Total votes\" = sum(Votes)) %&gt;%\n  datatable(rownames = F)\n\n\n\n\nTable 2: Total number of absolute and weight-corrected participants in the survey.\n\n\n\n\n\n\n\n\n\n\n\nThe cause of this discrepancy is unknown, but while collecting the data from the plots, I noticed that sometimes the total number of party votes do not add up, or a zero number of votes corresponded to a non-zero percentage (Figure 2). As a guess, I would say these small differences are the result of rounding errors. As a consequence, extracting the exact values from the plots in the article will be impossible. Luckily, these differences are small enough to have a negligible effect in further analyses.\n\n\n\n\n\n\nFigure 2: Breakdown of the weighted votes for N-VA (A) and Voor U (B) per faculty, as obtained from the university-wide plot in the Veto article. In A, the sum of all weighted votes does not equal 436, but 438 instead. In B, the faculty of Arts has zero votes, yet this somehow corresponds to 1.5% of the total.\n\n\n\nThe real question is: why is there even a difference between the absolute and weight-corrected votes at all? Weight correction - as the name implies - should only affect the weight of the votes between faculties, and not the total amount. As of yet, I have no explanation why in the Veto article the weight-corrected plot has 1728 participants, while summing absolute vote counts of the individual faculties yields 1801 participants.\n\n\n3.2 Non-weighted votes show a less prominent difference between the top-two parties\nHaving all the data, let’s recreate the university-wide plot with the non-corrected data. This can provide an insight the overall impact of the weight correction.\nOne remarkable element in the original plot with weight correction is the big difference between the two largest parties: N-VA scored 25.2%, while Groen came second with 16.8%, a difference of 8.4 percentage points! When looking at the raw voting results (Figure 3), this massive difference has melted away. N-VA still remains the largest party, but Groen and Vooruit follow closely, with a 1.4 and 2.7 percentage point difference respectively.\n\n\nShow code\n#Calculate the percentages of raw and weight-corrected votes for each party.\ndf_party_pct &lt;- df_votes %&gt;%\n  group_by(Party, Correction) %&gt;%\n  summarize(\"Votes - total\" = sum(Votes)) %&gt;%\n  group_by(Correction) %&gt;%\n  mutate(\"Percentage\" = `Votes - total`/sum(`Votes - total`))\n\n#Set the colors for each party. Muted colors for weighted, bright colors for non-corrected votes.\nparty_colors = c(\"#F1C77E\", \"#86C186\", \"#EC5D5B\", \"#719bba\", \"#FA8F61\", \"#BD0F0F\",\n                 \"#F5E78F\", \"#E28DC9\", \"#CCCCCC\", \"#CCCCCC\", \"#CCCCCC\", \"#CCCCCC\",\n                 \"#dc9417\", \"#4A904A\", \"#BF1A17\", \"#065788\", \"#E04300\", \"#7B0A0A\",\n                 \"#ECD332\", \"#CD349F\", \"#8B8B8B\", \"#8B8B8B\", \"#8B8B8B\", \"#8B8B8B\")\n\n#Plot.\nggplot(df_party_pct) +\n  geom_col(aes(x = Party, y = Percentage, fill = interaction(Party, Correction)), color = \"black\", position = position_dodge2(width = 0.9), show.legend = F) +\n  geom_text(aes(x = Party, y = Percentage - ifelse(Percentage &lt; 0.015, -0.015, 0.02), label = round(Percentage*100, 1)), position = position_dodge2(width = 0.9), angle = 90) +\n  scale_y_continuous(labels = scales::percent, breaks = seq(0, 0.3, 0.05)) +\n  scale_fill_manual(values = party_colors) +\n  coord_cartesian(ylim = c(0, 0.3)) +\n  theme_classic() +\n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_text(size = 18),\n        axis.text.x = element_text(size = 16, color = \"black\", angle = 45, hjust = 1),\n        axis.text.y = element_text(size = 16, color = \"black\")\n        )\n\n\n\n\n\n\n\n\nFigure 3: University-wide plot with the weight-corrected votes from the Veto article (muted colors, left), and the non-corrected votes (bright colors, right) that was obtained from the separate faculty plots.\n\n\n\n\n\nFrom Figure 3 , it seems that the weight correction had an opposite effect on the two largest parties: inflating the votes of N-VA, while reducing those of Groen, creating the massive gap between the two. The question now is: is the data that has been adjusted by the weight correction trustworthy?\n\n\n3.3 The rationale behind the weight correction\nFrom the description in the Veto article, we know that the correction was based on the relative sizes of the individual faculties, but what does this exactly mean?\nLet’s take the faculty of Science as an example: Table 1, states that 347 students of this faculty participated in the survey, and Table 2 shows that 1801 students participated in the survey. Consequently, the faculty of Science contributes 19.27% (= 347/1801) participants to the survey. However, with its 5249 students, the faculty only represents 7.12% of the total university student body. In the survey, the 1801 participants represent 100% of the university, and 7.12% of these should be from the faculty of Science, not 19.27%. As a result, the total participants are down-scaled from 347 (19.27% of 1801) to 129 (7.12% of 1801). These 129 participants are then redistributed according to the original voting results of the faculty (so for example: 23.3% for the party Groen now equals 31 “weight-corrected” people, and not the original 81).\nThe faculty of Science is an example where the number of participants far exceeds the relative size of the faculty within the university, and as a result undergoes a downward correction. Earlier I discussed two examples where the opposite happens: Economics and Medicine. Each representing 14%/15% of the university, but only 2.78%/5.05% of the survey, so they get a drastic upward correction.\nTo visualize this, the share of each faculty in survey participants is compared to the relative faculty size, and the faculty share in weight-corrected participants (Figure 4), making the aforementioned up-scaling effect of Medicine and Economics clearly visible. On the other side are the faculties of Science, Law and Criminology, and Arts. All three have been drastically down-scaled due to their high survey participant numbers (Figure 4).\n\n\nShow code\n#Calculate the relative proportions of absolute participants, weight-corrected participants and faculty size.\ndf_fac_tot &lt;- df_votes %&gt;%\n  group_by(Faculty, Abbreviation, `Students faculty`, Correction) %&gt;%\n  summarize(\"Total participants\" = sum(Votes)) %&gt;%\n  pivot_wider(names_from = Correction, values_from = `Total participants`) %&gt;%\n  ungroup() %&gt;%\n  mutate(\"Share of weight-corrected participants\" = weighted/sum(weighted),\n         \"Share of absolute participants\" = absolute/sum(absolute),\n         \"Relative size of faculty\" = `Students faculty`/sum(`Students faculty`)) %&gt;%\n  pivot_longer(cols = c(6:8), names_to = \"Type\", values_to = \"Percentage\")\n\n#Set variable Type as factor and set levels\ndf_fac_tot$Type &lt;- factor(df_fac_tot$Type, levels = c(\"Share of absolute participants\",\n                                                      \"Relative size of faculty\",\n                                                      \"Share of weight-corrected participants\"))\n\n#Rename the levels of the factor \"Type\" for a cleaner plot.\ndf_fac_tot$Type &lt;- fct_recode(df_fac_tot$Type,\n                                \"Share of \\n absolute participants\" = \"Share of absolute participants\",\n                                \"Relative size \\n of faculty\" = \"Relative size of faculty\", \n                                \"Share of weight- \\n corrected participants\" = \"Share of weight-corrected participants\")\n\n#Sort the levels of the factor \"Faculty\" according to the number of students.\ndf_fac_tot$Faculty &lt;- fct_reorder(df_fac_tot$Faculty, df_fac_tot$`Students faculty`, .desc = T)\n\n#Create two dataframes to add labels to the plot, even indexes of the factor \"Faculty\" on the left, and odd ones on the right.\ndf_labels_fac_even &lt;- df_fac_tot %&gt;%\n  filter(as.integer(Type) == 1) %&gt;%\n  arrange(desc(Faculty)) %&gt;%\n  mutate(\"CumSum\" = cumsum(Percentage) - Percentage/2) %&gt;%\n  filter(as.integer(Faculty)%%2 == 0)\n\ndf_labels_fac_odd &lt;- df_fac_tot %&gt;%\n  filter(as.integer(Type) == 3) %&gt;%\n  arrange(desc(Faculty)) %&gt;%\n  mutate(\"CumSum\" = cumsum(Percentage) - Percentage/2) %&gt;%\n  filter(as.integer(Faculty)%%2 != 0)\n\n#Plot.\nfac_colors &lt;- hue_pal()(16)\n\nggplot(df_fac_tot) +\n  geom_flow(aes(x = Type, y = Percentage, alluvium = Faculty, fill = Faculty), alpha = 0.25, color = \"#595655\", width = 0.3, curve_type = \"linear\", show.legend = F) +\n  geom_col(aes(x = Type, y = Percentage, fill = Faculty), width = 0.3, color = \"black\", show.legend = F) +\n  #Labels on the left.\n  geom_text_repel(data = df_labels_fac_even, aes(x = Type, y = CumSum, label = Abbreviation), max.overlaps = Inf, nudge_x = -0.75, segment.curvature = 0.1, segment.ncp = 3, segment.inflect = T, segment.square = F, segment.color = \"#6b696c\") +\n  #Labels on the right.\n  geom_text_repel(data = df_labels_fac_odd, aes(x = Type, y = CumSum, label = Abbreviation), max.overlaps = Inf, nudge_x = 0.75, segment.curvature = 0.1, segment.ncp = 3, segment.inflect = T, segment.square = F, segment.color = \"#6b696c\") +\n  scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, 0.2)) +\n  scale_fill_manual(values = fac_colors) +\n  theme_classic() +\n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_text(size = 18),\n        axis.text = element_text(size = 16, color = \"black\")\n        )\n\n\n\n\n\n\n\n\nFigure 4: Comparison of the share of survey participants, the relative sizes of each faculty and the weight-corrected participants. The weight reduction of faculties with high participant numbers such as Science and Arts is clearly visible. On the other hand, Medicine and Economics have been rescaled upwards quite dramatically.\n\n\n\n\n\nSince the relative size of the faculties is the basis of the weight correction, it is evident that the columns “Relative size of faculty” and “Share of weight-corrected participants” in Figure 4 are almost identical. The reason that they are not an exact match is probably because I used the KU Leuven 2022-2023 faculty numbers for calculating relative faculty size, while Veto might had access to a different source of student numbers for their weight correction.\n\n\n3.4 Several faculties have a high margin of error\nThe number of participants is extremely low for several faculties. For example, Cannon Law, Architecture, Pharmaceutical sciences and Theology all have less than 10 participants. Because of this, Veto added a warning to the faculty plots that the outcome of these faculties are probably not relevant (yet they still applied a weight correction and used the data for their university-wide plot).\nIn order to have a quantifiable measure of how reliable the faculty data are , I calculated the margin of error:\n\\[\n\\text{Margin of error} = z\\cdot\\sqrt{\\frac{p(1-p)}{N}}\n\\]\nWhere \\(z\\) is the z-score for a certain confidence level (1.96 is mostly used, and corresponds with a 95% confidence), \\(p\\) is the proportion of the result (i.e. the score of a party), and \\(N\\) the number of participants. Most surveys in (trustworthy) newspapers and websites will use a margin of error of ~3% at 95% confidence. This means that 95% of the time, the reported result of a party will lie within an interval of ±3 percentage points. As an example: a score of 20 ± 3% for Vooruit conveys that when you repeat the same survey 100 times, 95% of the obtained results will be between 17% and 23% for Vooruit. Hence, the smaller the margin of error, the more trustworthy the results.\nFor the Veto poll, I calculated the margin of error for the largest party per faculty (since \\(p\\) depends on the party score, the error will be larger for large parties and vice versa). Percentage scores were calculated using the non-corrected votes. The results can be found in Table 3.\n\n\nShow code\ndf_fac_party_pct &lt;- df_votes %&gt;%\n  group_by(Faculty, Correction) %&gt;%\n  slice_max(Votes, with_ties = F) %&gt;%\n  pivot_wider(names_from = Correction, values_from = c(Votes, Participants)) %&gt;%\n  ungroup() %&gt;%\n  mutate(\"Share of absolute participants\" = (Participants_absolute/sum(Participants_absolute)) %&gt;% round(., 4),\n         \"Share of weight-corrected participants\" = (Participants_weighted/sum(Participants_weighted)) %&gt;% round(., 4),\n         \"Percentage_vote_absolute\" = (Votes_absolute/Participants_absolute) %&gt;% round(., 4),\n         \"Error margin\" = (1.96*sqrt((Percentage_vote_absolute*(1 - Percentage_vote_absolute))/Participants_absolute)) %&gt;% round(., 4)) %&gt;%\n  select(Faculty, Abbreviation, Party, `Percentage_vote_absolute`, `Students faculty`, `Share of absolute participants`, `Share of weight-corrected participants`, `Error margin`) %&gt;%\n  arrange(desc(`Students faculty`))\n\ndatatable(df_fac_party_pct, filter = \"top\", rownames = F)\n\n\n\n\nTable 3: Margin of error on the largest party for each faculty.\n\n\n\n\n\n\n\n\n\n\n\nThe faculty of Law and Criminology has the smallest error margin (N-VA, 23.2 ± 4.27%), followed by Arts (Groen, 27.3 ± 4.37) and Science (Groen, 23.3 ± 4.45). Not surprisingly, all of these are faculties with a high number of participants. The median error of all faculties is 14%. Not a single faculty has an error below 3%, and four even exceeded 30% error. It is therefore clear that the survey data from most faculties is not statistically significant, and no sound conclusions can be drawn from it.\nTo visualize all of this, I plotted the share of participants of the survey versus the share of participants after weight correction, and assign the error margin to the size aesthetic:\n\n\nShow code\ndf_labels &lt;- df_fac_party_pct %&gt;%\n  filter(`Share of absolute participants` &gt; `Share of weight-corrected participants` + 0.03 | `Share of absolute participants` &lt; `Share of weight-corrected participants` - 0.03)\n\nggplot(df_fac_party_pct) +\n  geom_abline(slope = 1) +\n  geom_abline(slope = 1, intercept = 0.03, lty = 2) +\n  geom_abline(slope = 1, intercept = -0.03, lty = 2) +\n  geom_point(aes(x = `Share of absolute participants`, y = `Share of weight-corrected participants`, size = `Error margin`), pch = 21, fill = \"#29546a4D\") +\n  geom_text_repel(data = df_labels, aes(x = `Share of absolute participants`, y = `Share of weight-corrected participants`, label = paste0(Abbreviation, \" (\", Party, \")\")), box.padding = 0.75, segment.curvature = -0.1, segment.ncp = 3, segment.angle = 20, max.overlaps = Inf) +\n  scale_size_continuous(range = c(1, 12), labels = scales::percent) +\n  scale_x_continuous(labels = scales::percent) +\n  scale_y_continuous(labels = scales::percent) +\n  coord_cartesian(xlim = c(0, 0.25), ylim = c(0, 0.25)) +\n  theme_classic() +\n  theme(legend.position = \"bottom\",\n        axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16, color = \"black\"))\n\n\n\n\n\n\n\n\nFigure 5: Bubble plot of actual versus weight-corrected survey participant numbers per faculty. Bubbles above the first bisector denote large faculties with a low number of participants, which have been scaled up by the correction process (moved up along the y-axis). Points below the bisector are smaller faculties with high participant numbers, which have been scaled down after weight correction (moved down along the y-axis). The size of each bubble corresponds with the error margin on the result of the largest party in that faculty. Labels denote faculties that deviate more than 3 percentage points from the bisector (dotted lines)\n\n\n\n\n\nThe resulting plot in Figure 5 gives a good overview of everything that has been discussed up to now. Lots of faculties with high error margins have been scaled up by the weight correction ( Figure 5, large bubbles above the first bisector), inflating the weight of poor quality data. On the other hand, three faculties had an unusually high number of participants, but have been down-scaled by the weight correction, reducing the weight of decent - albeit still borderline - quality data (Figure 5, small bubbles below the first bisector).\nAs a consequence, the university-wide plot with weight correction, as made by Veto, unfortunately lacks any statistical power, and no real conclusions can be drawn from it."
  },
  {
    "objectID": "Personal_projects/posts/VetoPoll/VetoPoll.html#conclusions",
    "href": "Personal_projects/posts/VetoPoll/VetoPoll.html#conclusions",
    "title": "Some observations of the 2024 Veto federal election poll",
    "section": "4 Conclusions",
    "text": "4 Conclusions\nThe data acquired by the Veto survey in itself could have been very informative, if processed correctly. The writers of the article know very well that a survey with low number of participants yields data that cannot be completely trusted. They even warn for it in their own article, where faculty plots with low participant numbers are accompanied by the statement “The response for this faculty is very low, so take the results with a grain of salt.” So why bother correcting data of which you know it isn’t reliable in the first place? It reminded me of an anecdote recited by Charles Babbage, inventor of the first (mechanical) computing engines:\n\nOn two occasions I have been asked, “Pray, Mr. Babbage, if you put into the machine wrong figures, will the right answers come out?” … I am not able rightly to apprehend the kind of confusion of ideas that could provoke such a question.\nCharles Babbage\n\nOr more simply stated: garbage in, garbage out. Correcting low-quality data does not suddenly allow you to make statistically sound claims. Paradoxically, not correcting the data and presenting it as the voting preferences of 1801 people who happen to study at KU Leuven would have probably been the better approach.\nAs mentioned earlier, this survey was widely discussed in the media and politicized in the lead-up to the 2024 federal elections. Several newspapers reported on its conclusions, and De Standaard (De Smet, 2024) called upon the help of KU Leuven statistics professor Geert Molenberghs, who likewise stated that the number of participants is too low and differs too much between faculties to draw sound conclusions. In a reaction to this, the editor-in-chief of Veto responded:\n\n[The survey] is not worthless, because the trend is in line with expectations […]\nEditor-in-chief, Veto\n\nAs someone with experience in setting up scientific experiments and performing the necessary diligence to make them statistically sound, this statement irks me to no end. If the results confirm what you already expected, why then even bother doing a survey in the first place? This is a beautiful example of confirmation bias: the data fits my expectations, so it must be correct. With no statistical tests to confirm or deny it, the “trend” of which he speaks cannot be discerned from random noise. If the number of points is low enough, you will always be able to fit a line through them, one way or another.\nAll in all, the effort was good, and the authors did think about the right ways to correct for faculty weight, but it is a shame they applied it on data that is severely lacking in statistical power."
  },
  {
    "objectID": "Personal_projects/personal_projects.html",
    "href": "Personal_projects/personal_projects.html",
    "title": "Personal projects",
    "section": "",
    "text": "Some observations of the 2024 Veto federal election poll\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nSven Van Bael\n\n\n\n\n\n\n\n\n\n\n\n\nA closer look at Lego set prices\n\n\n\n\n\n\n\n\n\n\n\nJul 21, 2024\n\n\nSven Van Bael\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Academic_work/academic_work.html",
    "href": "Academic_work/academic_work.html",
    "title": "Academic work",
    "section": "",
    "text": "Calculating the net charge of a peptide\n\n\n\n\n\n\n\n\n\n\n\nJul 27, 2024\n\n\nSven Van Bael\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Academic_work/posts/PepNetCharge/PepNetCharge.html",
    "href": "Academic_work/posts/PepNetCharge/PepNetCharge.html",
    "title": "Calculating the net charge of a peptide",
    "section": "",
    "text": "This post describes a workflow for calculating the charge of a peptide from its amino acid composition. I start first with a quite extensive introduction on the origin of the formulas that are used. If you want to see the R code directly, go to Section 3."
  },
  {
    "objectID": "Academic_work/posts/PepNetCharge/PepNetCharge.html#packages-used",
    "href": "Academic_work/posts/PepNetCharge/PepNetCharge.html#packages-used",
    "title": "Calculating the net charge of a peptide",
    "section": "1 Packages used",
    "text": "1 Packages used\n\nlibrary(tidyverse) \nlibrary(DT)"
  },
  {
    "objectID": "Academic_work/posts/PepNetCharge/PepNetCharge.html#introduction",
    "href": "Academic_work/posts/PepNetCharge/PepNetCharge.html#introduction",
    "title": "Calculating the net charge of a peptide",
    "section": "2 Introduction",
    "text": "2 Introduction\nA lot of my academic work involved peptides, which are in essence short proteins that are typically around 5 to 20 amino acids long. A major part of my (post)doctoral project was using mass spectrometry to identify and quantify peptides in biological samples. A fundamental principle of mass spectrometry is that it measures the mass-to-charge ratios (m/z) of everything it was able to measure in a sample. Consequently, for quantification purposes I need to input the mass-to-charge ratios of my peptides of interest into the machine. While calculating the monoisotopic mass of a peptide is straightforward (summing the individual amino acid masses that make up the peptide), calculating the charge at a certain pH is slightly less trivial.\nThe method described below is based on two publications by Moore (1985) and Cameselle et al. (1986).\n\n2.1 Identifying all ionizable groups within a peptide\nContributing to the overall charge of a peptide are: the N-terminus (amino group), the C-terminus (carboxyl group), and the amino acids that have ionisable side chains: lysine (amino group), arginine (guanidino group), histidine (imidazole), glutamic acid (carboxyl group), aspartic acid (carboxyl group), cysteine (sulfhydryl) and tyrosine (phenol group). For amino acids, these can be divided into two types:\n\nGroups that have no charge at low pH and become negatively charged at high pH: carboxyl (COOH/COO-), sulfhydryl (SH/S-), and phenol (PhOH/PhO-).\nGroups that are positively charged at low pH and become neutral at high pH: amino (NH3+/NH3), guanidino (guanidino+/guanidino), and imidazole (imidazole+/imidazole).\n\nIf the charge of each of the aforementioned groups is known, the charge \\(Z\\) of a peptide at a given pH can simply be calculated by summing all the individual charges:\n\\[\n\\begin{align*}\nZ_{peptide} = Z_{N-term} &+ Z_{C-term}\n\\\\\n& + n\\cdot Z_{Lys}\n\\\\\n& + n\\cdot Z_{Arg}\n\\\\\n& + n\\cdot Z_{His}\n\\\\\n& + n\\cdot Z_{Glu}\n\\\\\n& + n\\cdot Z_{Asp}\n\\\\\n& + n\\cdot Z_{Cys}\n\\\\\n& + n\\cdot Z_{Tyr}\n\\end{align*}\n\\tag{1}\\]\nWhere \\(n\\) is the number of each corresponding amino acid present in the peptide. The question now is: how to calculate the charge \\(Z\\) for each element in question?\n\n\n2.2 Calculating the fraction of charged/neutral molecules\nThe Henderson-Hasselbalch equation states that the pH of a solution can be calculated from the acid dissociation constant (\\(pK_a\\))and the concentration of dissociated (\\(A^-\\)) and undissociated (\\(HA\\)) molecules.\n\\[\npH = pK_a + log \\frac{[A^-]}{[HA]}\n\\tag{2}\\]\nWhich can be rewritten as:\n\\[\n10^{pH-pK_a}=\\frac{[A^-]}{[HA]}\n\\tag{3}\\]\nIf you express the total number of molecules in solution as 100%, this means that \\([A^-]+[HA]=1\\) or, when rewritten: \\([HA]=1-[A^-]\\). Plugging this into Equation 3 results in:\n\\[\n10^{pH-pK_a} = \\frac{[A^-]}{1-[A^-]}\n\\tag{4}\\]\nRearranging for \\([A^-]\\) gives:\n\\[\n[A^-] = \\frac{1}{10^{pK_a-pH} + 1}\n\\tag{5}\\]\nAnd since it was earlier established that \\([HA] = 1 - [A^-]\\):\n\\[\n\\begin{align*}\n[HA] &= 1 - \\frac{1}{10^{pK_a-pH}+1}\n\\\\\n[HA] &= \\frac{1}{10^{pH-pK_a}+1}\n\\end{align*}\n\\tag{6}\\]\n\n\n2.3 Using the \\([A^-]\\) and \\([HA]\\) fractions to calculate overall charge\nIn Section 2.1, the distinction was made between the ionizable groups within a peptide that have a negative charge (-1) when dissociated versus those that have a positive charge (+1) when undissociated. Using the formulas derived from the Henderson-Hasselbalch equation, each fraction can now be calculated.\nLet’s take the C-terminus as an example: the \\(pK_a\\) of an amino acid carboxyl group is ~3.5, and has a negative charge when dissociated, so the formula for \\([A^-]\\) is used to calculate the fraction of dissociated molecules. With a pH of 7 as example, this gives:\n\\[\n\\begin{align*}\n[A^-] &= \\frac{1}{10^{pK_a-pH}+1}\n\\\\\n&= \\frac{1}{10^{3.5-7}+1}\n\\\\\n&= 99.9\\%\n\\end{align*}\n\\tag{7}\\]\nSo 99.9% of the time, the carboxyl group is in the dissociated form (COO-) at pH = 7. Since the charge of a dissociated carboxyl group equals -1, the mean charge \\(Z_{C-term}\\) is \\(0.999 \\times -1 = -0.999\\) at pH = 7. Note that the 0.01% that is in the undissociated COOH form is not ionized, and therefore does not contribute to the overall charge.\nThe same principle can be applied to the N-terminus, but as this group only carries a charge when undissociated, the formula for \\([HA]\\) is used:\n\\[\n\\begin{align*}\n[HA] &= \\frac{1}{10^{pH-pK_a}+1}\n\\\\\n&= \\frac{1}{10^{7-7.5} + 1}\n\\\\\n&= 76\\%\n\\end{align*}\n\\tag{8}\\]\nWith 76% of the molecules in the positively charged nondissociated form (NH3+), the mean charge \\(Z_{N-term}\\) equals \\(0.76 \\times (+1) = 0.76\\) at pH = 7. Again, the 24% that are in the dissociated NH2 form do not carry a charge, and therefore do not contribute.\nCalculating the charges carried by the amino acid side chains is analogous. For this example, the peptide has no amino acids with ionizable side chains, so Equation 1 can be simplified to calculate the overall charge of the peptide at pH = 7:\n\\[\n\\begin{align*}\nZ_{peptide} &= Z_{N-term} + Z_{C-term}\n\\\\\n&= 0.76 + (-0.999)\n\\\\\n&= -0.239\n\\end{align*}\n\\tag{9}\\]\nSince the outcomes of Equation 7 and 8 still needed to be multiplied by the value 1 or -1 for positively or negatively charged ions, Equation 5 and 6 can be generalized as:\n\\[\n\\begin{align*}\nZ^- &= \\frac{-1}{10^{pK_a-pH}+1}\n\\\\\nZ^+ &= \\frac{+1}{10^{pH-pK_a}+1}\n\\end{align*}\n\\tag{10}\\]\n\n\n2.4 The pKa values of the termini and amino acid side chains\nThere are different sources available for the pKa values of N- and C-termini of amino acids and their side chains. From experience, the resource provided by Bjellqvist et al. (1993) has given the best results, with predicted charge states that correspond well with my own empirically obtained data.\n\ndf_bjellqvist &lt;- read_csv(\"pK values - Bjellqvist.csv\")\ndf_bjellqvist &lt;- column_to_rownames(df_bjellqvist, var = \"Single-letter code\")\ndatatable(df_bjellqvist)\n\n\n\nTable 1: Bjellqvist pKa values for termini and side chains of all 20 amino acids."
  },
  {
    "objectID": "Academic_work/posts/PepNetCharge/PepNetCharge.html#sec-writing-the-r-function",
    "href": "Academic_work/posts/PepNetCharge/PepNetCharge.html#sec-writing-the-r-function",
    "title": "Calculating the net charge of a peptide",
    "section": "3 Creating the R function",
    "text": "3 Creating the R function\nUsing the formulas in 10 and the pKa information from Bjellqvist, I created the function computeCharge that calculates the net charge of a peptide at a given pH.\n\ncomputeCharge &lt;- function(Peptide, pH){\n  #Get amino acid composition from peptide\n  pept &lt;- unlist(str_split(Peptide, \"\"))\n  if (sum(!pept %in% rownames(df_bjellqvist)) &gt; 0){\n    #Abort function when the Peptide string contains letters that do not correspond with an amino acid.\n    warning(\"Peptide string contains unknown amino acid character(s). Please check.\")\n  } else {\n    compoAA &lt;- pept %&gt;%\n    factor(., levels = LETTERS) %&gt;%\n    table()\n  \n    #Get the N-terminal and C-terminal amino acid\n    nTermAA &lt;- pept[1]\n    cTermAA &lt;- pept[length(pept)]\n  \n    #Calculate charge for N- and C-termini and amino acid residues with the pKa information from df_bjellqvist\n    cter &lt;- -1/(10^(df_bjellqvist[cTermAA, \"pK1\"] - pH) + 1)\n    nter &lt;- 1/(10^(pH - df_bjellqvist[nTermAA, \"pK2\"]) + 1)\n  \n    carg &lt;- as.vector(compoAA['R'] * (1/(10^(pH - df_bjellqvist[\"R\", \"pKr\"]) + 1)))\n    chis &lt;- as.vector(compoAA['H'] * (1/(10^(pH - df_bjellqvist[\"H\", \"pKr\"]) + 1)))\n    clys &lt;- as.vector(compoAA['K'] * (1/(10^(pH - df_bjellqvist[\"K\", \"pKr\"]) + 1)))\n  \n    casp &lt;- as.vector(compoAA['D'] * (-1/(10^(df_bjellqvist['D', \"pKr\"] - pH) + 1)))\n    cglu &lt;- as.vector(compoAA['E'] * (-1/(10^(df_bjellqvist['E', \"pKr\"] - pH) + 1)))\n    ccys &lt;- as.vector(compoAA['C'] * (-1/(10^(df_bjellqvist['C', \"pKr\"] - pH) + 1)))\n    ctyr &lt;- as.vector(compoAA['Y'] * (-1/(10^(df_bjellqvist['Y', \"pKr\"] - pH) + 1)))\n  \n    charge &lt;- cter + casp + cglu + ccys + ctyr + nter + carg + chis + clys\n    return(charge)\n  }\n}\n\nLet’s test the function using the peptide sequence DGLDAASYYAPVR, which is part of a standard for retention time calibration (Escher et al., 2012). Typically, samples for mass spectrometric analyses are dissolved in a highly acidic buffer containing 0.1% formic acid, which has a pH of ~2.7, so this value will be used as input.\n\ncomputeCharge(\"DGLDAASYYAPVR\", 2.7)\n## [1] 1.790697\n\nThe value of 1.79 can be interpreted that there both are molecules with charge +1 and charge +2 present, but since the calculated charge is closer to +2, the vast majority will be doubly charged. If necessary, the fraction \\(f\\) of doubly charged molecules can be calculated as follows:\n\\[\n\\begin{align*}\nf \\times (+2) + (1-f) \\times (+1) &= 1.79\n\\\\\n2f - f + 1 &= 1.79\n\\\\\nf &= 0.79\n\\end{align*}\n\\]\nHence, 79% of the molecules are doubly charged while 21% (1 - 0.79 = 0.21) are singly charged.\nIn a second example, let’s test the peptide ECCHGDLLECADDR, which originates from the bovine serum albumin protein (BSA).\n\ncomputeCharge(\"ECCHGDLLECADDR\", 2.7)\n## [1] 2.712474\n\nAgain, the majority of this peptide will be triply charged. The exact ratios can be calculated as before, but now for +2 and +3 charges:\n\\[\n\\begin{align*}\nf \\times (+3) + (1 - f) \\times (+2) &= 2.71\n\\\\\n3f+2-2f &= 2.71\n\\\\\nf &= 0.71\n\\end{align*}\n\\]\nSo 71% of this peptide occurs as triply charged, with 29% being doubly charged."
  },
  {
    "objectID": "Academic_work/posts/PepNetCharge/PepNetCharge.html#creating-a-charge-profile-along-the-ph-scale",
    "href": "Academic_work/posts/PepNetCharge/PepNetCharge.html#creating-a-charge-profile-along-the-ph-scale",
    "title": "Calculating the net charge of a peptide",
    "section": "4 Creating a charge profile along the pH scale",
    "text": "4 Creating a charge profile along the pH scale\nIn some situations, it can be interesting to see how the charge of a peptide changes over the pH range. For this purpose, I used my computeCharge function to plot a charge profile. As an example peptide, I used EAVSEILETSR, a peptide tag described by Vandemoortele et al. (2016).\n\n#Define example peptide\npept_1 &lt;- \"EAVSEILETSR\"\n\n#Iterate the computeCharge function over the entire pH scale\ndf_pH &lt;- tibble(\"pH\" = seq(0, 14, 0.2),\n                \"Charge\" = sapply(pH, computeCharge, Peptide = pept_1))\n\n#Set maximum of the charge axis\ny_scale &lt;- max(abs(df_pH$Charge)) %&gt;% ceiling()\n\n#Plot\nggplot(df_pH) +\n  geom_hline(yintercept = 0, color = \"#565253\", lty = 2) +\n  geom_vline(xintercept = 7, color = \"#565253\", lty = 2) +\n  geom_line(aes(x = pH, y = Charge), color = \"#033e57\") +\n  scale_x_continuous(limits = c(0, 14), breaks = seq(0, 14, 1)) +\n  scale_y_continuous(limits = c(-y_scale, y_scale), breaks = seq(-y_scale, y_scale, 1)) +\n  labs(title = pept_1) +\n  theme_classic() +\n  theme(axis.text = element_text(size = 16, color = \"black\"),\n        axis.title = element_text(size = 18, face = \"bold\"),\n        plot.title = element_text(hjust = 0.5, size = 18, face = \"bold\"))\n\n\n\n\n\n\n\nFigure 1: Charge profile of EAVSEILETSR along the pH scale"
  },
  {
    "objectID": "Personal_projects/posts/LegoSetPrices/LegoSetPrices.html",
    "href": "Personal_projects/posts/LegoSetPrices/LegoSetPrices.html",
    "title": "A closer look at Lego set prices",
    "section": "",
    "text": "Many people will be familiar with Lego, the popular construction sets with interlocking brightly coloured plastic bricks. In the later years, Lego marketing has successfully targeted a more adult audience (including myself), with sets that are more intricate and detailed. Every Lego fan, or who knows Lego fans (be it a child or an adult) will also know the sometimes hefty price ranges, especially for the “adult” sets, and those under an intellectual property license (think of Star Wars, Marvel, Minecraft, etc.).\nThanks to the R package brickset I was able to collect data on all Lego sets released during 2018 and 2024, with variables such as set theme, intellectual property license, number of pieces, retail price, etc. By exploring this data set I was curious if I could find an answer to some questions:\n\nAre smaller sets more expensive to produce than bigger ones, i.e. is there a difference in price-to-part ratio?\nIs there a big price difference between intellectual property licensed sets and other sets?\nWhich intellectual property has the most expensive Lego sets?\nWhich set has the lowest price-to-part ratio?"
  },
  {
    "objectID": "Personal_projects/posts/LegoSetPrices/LegoSetPrices.html#introduction",
    "href": "Personal_projects/posts/LegoSetPrices/LegoSetPrices.html#introduction",
    "title": "A closer look at Lego set prices",
    "section": "",
    "text": "Many people will be familiar with Lego, the popular construction sets with interlocking brightly coloured plastic bricks. In the later years, Lego marketing has successfully targeted a more adult audience (including myself), with sets that are more intricate and detailed. Every Lego fan, or who knows Lego fans (be it a child or an adult) will also know the sometimes hefty price ranges, especially for the “adult” sets, and those under an intellectual property license (think of Star Wars, Marvel, Minecraft, etc.).\nThanks to the R package brickset I was able to collect data on all Lego sets released during 2018 and 2024, with variables such as set theme, intellectual property license, number of pieces, retail price, etc. By exploring this data set I was curious if I could find an answer to some questions:\n\nAre smaller sets more expensive to produce than bigger ones, i.e. is there a difference in price-to-part ratio?\nIs there a big price difference between intellectual property licensed sets and other sets?\nWhich intellectual property has the most expensive Lego sets?\nWhich set has the lowest price-to-part ratio?"
  },
  {
    "objectID": "Personal_projects/posts/LegoSetPrices/LegoSetPrices.html#packages-used",
    "href": "Personal_projects/posts/LegoSetPrices/LegoSetPrices.html#packages-used",
    "title": "A closer look at Lego set prices",
    "section": "2 Packages used",
    "text": "2 Packages used\n\nlibrary(brickset)\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(ggbeeswarm)\nlibrary(ggrepel)"
  },
  {
    "objectID": "Personal_projects/posts/LegoSetPrices/LegoSetPrices.html#obtaining-data-on-all-lego-sets-released-from-2018-to-2024",
    "href": "Personal_projects/posts/LegoSetPrices/LegoSetPrices.html#obtaining-data-on-all-lego-sets-released-from-2018-to-2024",
    "title": "A closer look at Lego set prices",
    "section": "3 Obtaining data on all Lego sets released from 2018 to 2024",
    "text": "3 Obtaining data on all Lego sets released from 2018 to 2024\nThe getSets function from the brickset package interfaces with the API on the Brickset website (an active account on this website is required) and returns a data frame containing all Lego sets released for a specified year. Initially, I was planning to create a database with all sets released in the last 5 years (2019-2024). However, being alert for potential COVID-19 related effects (production lags, delayed set releases, …), I instead opted for the 2018-2024 period to have a good amount of pre- and post-COVID-19 data. The combined data frame is available as Lego_sets_18to24.csv\n\ndf_sets_18to24 &lt;- read_csv(\"Lego_sets_18to24.csv\")\nglimpse(df_sets_18to24)\n## Rows: 6,085\n## Columns: 36\n## $ setID                 &lt;dbl&gt; 27724, 27828, 27829, 27830, 28316, 27454, 27455,…\n## $ number                &lt;chr&gt; \"10260\", \"10261\", \"10262\", \"10263\", \"10268\", \"10…\n## $ numberVariant         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n## $ name                  &lt;chr&gt; \"Downtown Diner\", \"Roller Coaster\", \"James Bond …\n## $ year                  &lt;dbl&gt; 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, …\n## $ theme                 &lt;chr&gt; \"Creator Expert\", \"Creator Expert\", \"Creator Exp…\n## $ themeGroup            &lt;chr&gt; \"Model making\", \"Model making\", \"Model making\", …\n## $ subtheme              &lt;chr&gt; \"Modular Buildings Collection\", \"Fairground Coll…\n## $ category              &lt;chr&gt; \"Normal\", \"Normal\", \"Normal\", \"Normal\", \"Normal\"…\n## $ released              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, …\n## $ pieces                &lt;dbl&gt; 2480, 4124, 1290, 1166, 826, 85, 186, 295, 579, …\n## $ minifigs              &lt;dbl&gt; 6, 11, NA, 8, 3, NA, 1, 2, 2, 3, NA, NA, NA, NA,…\n## $ bricksetURL           &lt;chr&gt; \"https://brickset.com/sets/10260-1\", \"https://br…\n## $ rating                &lt;dbl&gt; 4.3, 4.3, 4.2, 3.9, 4.0, 3.7, 3.9, 3.8, 4.0, 4.1…\n## $ reviewCount           &lt;dbl&gt; 7, 5, 10, 1, 3, 3, 2, 0, 2, 2, 1, 1, 2, 1, 1, 0,…\n## $ packagingType         &lt;chr&gt; \"Box\", \"Box\", \"Box\", \"Box\", \"Box\", \"Box\", \"Box\",…\n## $ availability          &lt;chr&gt; \"LEGO exclusive\", \"LEGO exclusive\", \"LEGO exclus…\n## $ agerange_min          &lt;dbl&gt; 16, 16, NA, 12, 12, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4…\n## $ thumbnailURL          &lt;chr&gt; \"https://images.brickset.com/sets/small/10260-1.…\n## $ imageURL              &lt;chr&gt; \"https://images.brickset.com/sets/images/10260-1…\n## $ US_retailPrice        &lt;dbl&gt; 169.99, 379.99, 149.99, 99.99, 199.99, 4.99, 9.9…\n## $ US_dateFirstAvailable &lt;dttm&gt; 2018-01-01, 2018-06-02, 2018-08-01, 2018-10-02,…\n## $ US_dateLastAvailable  &lt;dttm&gt; 2020-11-24, 2021-11-20, 2022-01-27, 2020-11-20,…\n## $ UK_retailPrice        &lt;dbl&gt; 129.99, 299.99, 129.99, 84.99, 159.99, 4.99, 8.9…\n## $ UK_dateFirstAvailable &lt;dttm&gt; 2018-01-01, 2018-06-02, 2018-08-01, 2018-10-02,…\n## $ UK_dateLastAvailable  &lt;dttm&gt; 2020-11-04, 2021-11-07, 2021-08-02, 2021-03-02,…\n## $ CA_retailPrice        &lt;dbl&gt; 219.99, 479.99, 179.99, 129.99, 249.99, 5.99, 12…\n## $ CA_dateFirstAvailable &lt;dttm&gt; 2018-01-01, 2018-06-02, 2018-08-01, 2018-10-02,…\n## $ CA_dateLastAvailable  &lt;dttm&gt; 2020-11-24, 2021-11-20, 2022-01-27, 2020-11-20,…\n## $ DE_retailPrice        &lt;dbl&gt; 149.99, 329.99, 149.99, 87.72, 175.45, 4.99, 9.9…\n## $ DE_dateFirstAvailable &lt;dttm&gt; 2018-02-02, 2018-06-02, 2018-08-01, 2018-10-02,…\n## $ DE_dateLastAvailable  &lt;dttm&gt; 2020-11-05, 2021-11-15, 2021-08-04, 2020-12-11,…\n## $ height                &lt;dbl&gt; 37.5, 48.6, 28.2, 28.0, 47.5, 9.1, 14.1, 19.1, 2…\n## $ width                 &lt;dbl&gt; 58.0, 58.1, 48.0, 47.7, 37.5, 12.2, 15.7, 26.2, …\n## $ depth                 &lt;dbl&gt; 9.80, 18.60, 9.10, 8.70, 10.70, 5.90, 6.10, 4.60…\n## $ weight                &lt;dbl&gt; NA, 5.800, NA, 1.365, 2.156, NA, NA, NA, NA, NA,…"
  },
  {
    "objectID": "Personal_projects/posts/LegoSetPrices/LegoSetPrices.html#cleaning-the-dataset",
    "href": "Personal_projects/posts/LegoSetPrices/LegoSetPrices.html#cleaning-the-dataset",
    "title": "A closer look at Lego set prices",
    "section": "4 Cleaning the dataset",
    "text": "4 Cleaning the dataset\nYou can see that the data frame has a total of 36 columns with an abundance of information that is not useful for this particular analysis. So for the next steps, I will only select the columns setID, number, name, year, theme, themeGroup, subtheme, category, released, pieces, and DE_retailPrice (I’m located in Europe, so I opted for the prices in Euro).\n\ndf_sets_18to24 &lt;- read_csv(\"Lego_sets_18to24.csv\") %&gt;%\n  select(setID, number, name, year, theme, themeGroup, subtheme, category, released, pieces, DE_retailPrice)\nhead(df_sets_18to24, n = 20) %&gt;%\n  datatable(rownames = F)\n\n\n\nTable 1: First 20 rows of sets_18to24.\n\n\n\n\n\n\n\n\n\n\n\nTo make things easier later on, any NAs in subtheme are filled in by the value of the column themeGroup:\n\ndf_sets_18to24$subtheme &lt;- ifelse(is.na(df_sets_18to24$subtheme) == T,\n                                  df_sets_18to24$themeGroup,\n                                  df_sets_18to24$subtheme)\n\nWhile exploring the data further, it seems that the information on intellectual property license is spread over both columns themeGroup or subtheme, with no overlap between the two. As I want to be able to make an easy distinction between licensed and not-licensed, the new column License is created that contains this information. Additionally, the following filters are applied on the data frame:\n\npieces cannot be NA, and is cut-off at 10, because very small sets are often specialized expansion sets such as base plates, road plates, rails, etc.\nDE_retailPrice cannot be NA.\nAs I only focus on actual Lego sets, theme and subtheme cannot be “Duplo”, and category cannot be “Book”.\nreleased equals TRUE, so it only contains sets that have been released on the market.\n\nAnd finally, the column Price-to-part ratio is added, which is calculated from DE_retailPrice and pieces.\n\ndf_sets_18to24_filt &lt;- df_sets_18to24 %&gt;%\n  filter(is.na(pieces) == FALSE,\n         pieces &gt; 10,\n         is.na(DE_retailPrice) == FALSE,\n         theme != \"Duplo\",\n         subtheme != \"Duplo\",\n         category != \"Book\",\n         released == TRUE\n         ) %&gt;%\n  mutate(\"License\" = ifelse(themeGroup == \"Licensed\"|subtheme == \"Licensed\",\n                            \"Licensed\",\n                            \"No license\"),\n         \"Price-to-part ratio\" = DE_retailPrice/pieces)\n\nThe column year is converted into type factor, instead of type double. This will turn it from a continuous variable into a categorical variable, and is necessary when using year as a grouping variable, or when creating plots with ggplot2. Also, the License column is turned into type factor, with the ordered levels “No license” and “License”.\n\ndf_sets_18to24_filt$year &lt;- factor(df_sets_18to24_filt$year)\ndf_sets_18to24_filt$License &lt;- factor(df_sets_18to24_filt$License, levels = c(\"No license\", \"Licensed\"))\n\nhead(df_sets_18to24_filt, n = 20) %&gt;%\n  datatable(rownames = F)\n\n\n\nTable 2: First 20 rows of sets_18to24_filt."
  },
  {
    "objectID": "Personal_projects/posts/LegoSetPrices/LegoSetPrices.html#visually-exploring-the-data",
    "href": "Personal_projects/posts/LegoSetPrices/LegoSetPrices.html#visually-exploring-the-data",
    "title": "A closer look at Lego set prices",
    "section": "5 Visually exploring the data",
    "text": "5 Visually exploring the data\nAll code that is used to generate each figure and table is available under “Show code”.\n\n5.1 Number of sets released each year\n\n\nShow code\n#Calculate the number of sets released each year.\ndf_release_yr &lt;- df_sets_18to24_filt %&gt;%\n  group_by(year, License) %&gt;%\n  summarise(\"n_sets\" = n()) %&gt;%\n  mutate(\"All\" = sum(n_sets)) %&gt;%\n  pivot_wider(names_from = License, values_from = n_sets) %&gt;%\n  pivot_longer(cols = c(Licensed, `No license`, All), names_to = \"License\", values_to = \"n_sets\")\n\ndf_release_yr$License &lt;- factor(df_release_yr$License, levels = c(\"No license\", \"Licensed\", \"All\"))\n\n#Plot.\nggplot(df_release_yr) +\n  geom_point(aes(x = year, y = n_sets, color = License, fill = License), pch = 21, size = 3) +\n  geom_line(aes(x = year, y = n_sets, color = License, group = License), linewidth = 1) +\n  scale_x_discrete(name = \"Year\") +\n  scale_y_continuous(name = \"Number of sets\") +\n  coord_cartesian(ylim = c(0, 390)) +\n  scale_color_manual(values = c(\"#9a1917\", \"#d88c0f\", \"#000000\")) +\n  scale_fill_manual(values = c(\"#9a19174D\", \"#d88c0f4D\", \"#0000004D\")) +\n  theme_classic() +\n  theme(axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16, color = \"black\"),\n        legend.position = \"bottom\",\n        legend.title = element_blank(),\n        legend.text = element_text(size = 14))\n\n\n\n\n\n\n\n\nFigure 1: Number of non-licensed, licensed, and all sets released from 2018 to 2024.\n\n\n\n\n\nIn Figure 1 you can see that the number of sets released each year is quite stable, with almost an equal number of licensed versus non-licensed sets being released. Interesting is the difference in 2020, where there is a drop in licensed sets, and an increase in the non-licensed ones. Not sure what has caused this difference here, but a good guess is that due to COVID-19 a lot of media releases (movies, tv series, …) have been postponed, which in turn could have resulted in the accompanying Lego sets not being released as well?\nAlso noteworthy is that the number of sets for the years 2023 and 2024 is almost equal. However, the data for 2024 is incomplete, as at the time of writing it is only July 2024, meaning that in 7 months Lego has released almost as many sets as in the entire year of 2023.\n\n\n5.2 Evolution of set sizes and retail prices from 2018 to 2024\n\n\nShow code\n#Calculate the mean piece count of all sets for each year.\ndf_pieces_stats &lt;- df_sets_18to24_filt %&gt;%\n  group_by(year, License) %&gt;%\n  summarize(\"mean pieces\" = mean(pieces),\n            \"sd\" = sd(pieces))\n\n#Plot.\nggplot(df_sets_18to24_filt) +\n  facet_grid(~year) +\n  geom_quasirandom(aes(x = License, y = pieces, color = License, fill = License), pch = 21, size = 3) +\n  geom_point(data = df_pieces_stats, aes(x = License, y = `mean pieces`), size = 3) +\n  scale_y_continuous(name = \"Piece count\", trans = \"log10\", breaks = 10^seq(0, 4, 1)) +\n  scale_color_manual(values = c(\"#9a1917\", \"#d88c0f\")) +\n  scale_fill_manual(values = c(\"#9a19174D\", \"#d88c0f4D\")) +\n  coord_cartesian(ylim = c(10^0, 10^4)) +\n  theme_classic() +\n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_text(size = 18),\n        axis.text.x = element_blank(),\n        axis.text.y = element_text(size = 16, color = \"black\"),\n        legend.position = \"bottom\",\n        legend.title = element_blank(),\n        legend.text = element_text(size = 14),\n        strip.text = element_text(size = 14, face = \"bold\"))\n\n\n\n\n\n\n\n\nFigure 2: Piece count (axis in log scale) of non-licensed and licensed sets from 2018 to 2024.\n\n\n\n\n\nFigure 2 shows that there is no substantial difference in piece count between licensed and non-licensed sets. Interesting is that the mean piece count of Lego sets has been increasing over the years. In 2018, the mean piece count was 404/430 for non-licensed/licensed sets respectively. By 2024, this has increased to 737/627 for non-licensed/licensed. As a consequence, mean set prizes should have also increased in that time period.\n\n\nShow code\n#Calculate the mean retail price of all sets for each year.\ndf_prices_stats &lt;- df_sets_18to24_filt %&gt;%\n  group_by(year, License) %&gt;%\n  summarize(\"mean retail price\" = mean(DE_retailPrice),\n            \"sd\" = sd(DE_retailPrice))\n\n#Plot\nggplot(df_sets_18to24_filt) +\n  facet_grid(~year) +\n  geom_quasirandom(aes(x = License, y = DE_retailPrice, color = License, fill = License), pch = 21, size = 3) +\n  geom_point(data = df_prices_stats, aes(x = License, y = `mean retail price`), size = 3) +\n  scale_y_continuous(name = \"Retail price (EUR)\", trans = \"log10\") +\n  scale_color_manual(values = c(\"#9a1917\", \"#d88c0f\")) +\n  scale_fill_manual(values = c(\"#9a19174D\", \"#d88c0f4D\")) +\n  coord_cartesian(ylim = c(10^0, 10^3)) +\n  theme_classic() +\n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_text(size = 18),\n        axis.text.x = element_blank(),\n        axis.text.y = element_text(size = 16, color = \"black\"),\n        legend.position = \"bottom\",\n        legend.title = element_blank(),\n        legend.text = element_text(size = 14),\n        strip.text = element_text(size = 14, face = \"bold\"))\n\n\n\n\n\n\n\n\nFigure 3: Retail prices (axis in log scale) of non-licensed and licensed sets from 2018 to 2024.\n\n\n\n\n\nAs expected, in Figure 3 the mean retail price for sets has increased from 38.57 EUR/45.27 EUR (non-licensed/licensed) in 2018 to 64.85 EUR/66.64 EUR (non-licensed/licensed). This corresponds to the increase in set piece count seen in Figure 2.\n\n\n5.3 Evolution of the price-to-part ratio from 2018 to 2024\nThe most simple way to take into account the effect of the set piece count on the retail price is to calculate the price-to-part ratio.\n\n\nShow code\n#Calculate mean price-to-part ratio for each year.\ndf_ptp_stats &lt;- df_sets_18to24_filt %&gt;%\n  group_by(year) %&gt;%\n  mutate(\"mean PtP ratio - all\" = mean(mean(`Price-to-part ratio`))) %&gt;%\n  group_by(year, License) %&gt;%\n  summarize(\"mean PtP ratio\" = mean(`Price-to-part ratio`),\n            \"median PtP ratio\" = median(`Price-to-part ratio`),\n            \"sd\" = sd(`Price-to-part ratio`))\n\n#Plot.\nggplot(df_sets_18to24_filt) +\n  facet_grid(~year) +\n  geom_quasirandom(aes(x = License, y = `Price-to-part ratio`, color = License, fill = License), dodge.width = 0.5, pch = 21, size = 3) +\n  geom_point(data = df_ptp_stats, aes(x = License, y = `mean PtP ratio`), size = 3) +\n  geom_errorbar(data = df_ptp_stats, aes(x = License, ymin = `mean PtP ratio` - sd, ymax = `mean PtP ratio` + sd), width = 0.2) +\n  scale_color_manual(values = c(\"#9a1917\", \"#d88c0f\")) +\n  scale_fill_manual(values = c(\"#9a19174D\", \"#d88c0f4D\")) +\n  theme_classic() +\n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_text(size = 18),\n        axis.text.x = element_blank(),\n        axis.text.y = element_text(size = 16, color = \"black\"),\n        legend.position = \"bottom\",\n        legend.title = element_blank(),\n        legend.text = element_text(size = 14),\n        strip.text = element_text(size = 14, face = \"bold\"))\n\n\n\n\n\n\n\n\nFigure 4: Price-to-part ratio (in EUR per piece) of non-licensed and licensed sets from 2018 to 2024.\n\n\n\n\n\nIn general, the price-to-part ratio for licensed sets is indeed slightly higher than those of non-licensed sets, with 2018 being the only exception (Figure 4). While most values clump together, there are some outliers with price-to-part ratios going as high as 1 EUR. Let’s take a closer look by filtering for sets with a price-to-part ratio higher than 0.5 EUR.\n\n\nShow code\n#Filter for sets that have a price-to-part ratio higher than 0.5 EUR\ndf_high_ptp_sets &lt;- filter(df_sets_18to24_filt, `Price-to-part ratio` &gt; 0.5)\ndatatable(df_high_ptp_sets, rownames = F,filter = \"top\")\n\n\n\n\nTable 3: Lego sets with a price-to-part ratio over 0.5 EUR per piece.\n\n\n\n\n\n\n\n\n\n\n\n\nsummary(df_high_ptp_sets$License)\n## No license   Licensed \n##         14          7\nsummary(factor(df_high_ptp_sets$subtheme))\n##      Playmats Power-Up Pack         SPIKE        Stuntz        Trains \n##             3             7             2             8             1\n\nInterestingly, Table 3 contains more non-licensed sets than licensed ones (which are thought to be more expensive). When looking at the subthemes, you can see that the table contains three playmat sets, and one train track extension. These are non-standard sets with non-standard pieces, explaining the high price-to-part ratios. The same reasoning can explain the high ratios for sets of the SPIKE educational theme (aimed at learning programming to kids, contains specialized parts such as motors, sensors, etc.) and the Lego City Stuntz theme (containing flywheel-powered stunt bikes). Finally, there is the Super Mario Power-Up Pack theme, an interactive playset that combines Super Mario gameplay with Lego bricks. As these sets are both licensed and contain specialized electronic parts, it is one of the most expensive in this list, with several sets reaching 0.9 EUR per brick.\nWhen doing a deeper analysis, it could be argued that playmats, train track extensions and educational sets with expensive electronic parts should be excluded when analyzing the pricing of more standard Lego sets. For now, I will keep in mind that 21 sets have very aberrant price-to-part ratios, but that this will probably have little impact on the total of 2526 sets in df_sets_18to24_filt.\nNext, let’s focus on how the mean price-to-part ratio (also shown in Figure 4 as black dots) evolves over the years for both licensed and non-licensed sets:\n\n\nShow code\nggplot(df_ptp_stats) +\n  geom_line(aes(x = year, y = `mean PtP ratio`, color = License, group = License), linewidth = 1) +\n  geom_point(aes(x = year, y = `mean PtP ratio`, color = License, fill = License), pch = 21, size = 3) +\n  scale_x_discrete(name = \"Year\") +\n  scale_y_continuous(name = \"Mean price-to-part ratio\") +\n  coord_cartesian(ylim = c(0, 0.15)) +\n  scale_color_manual(values = c(\"#9a1917\", \"#d88c0f\")) +\n  scale_fill_manual(values = c(\"#9a19174D\", \"#d88c0f4D\")) +\n  theme_classic() +\n  theme(axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16, color = \"black\"),\n        legend.position = \"bottom\",\n        legend.title = element_blank(),\n        legend.text = element_text(size = 14))\n\n\n\n\n\n\n\n\nFigure 5: Evolution of the mean price-to-part ratio (in EUR per piece) of non-licensed and licensed sets from 2018 to 2024.\n\n\n\n\n\nIn Figure 5 you can see that - except for 2018 - licensed sets have always been more expensive than non-licensed sets, with the difference ranging from 1.1 cents (2021) to 1.6 cents (2023) per piece. Both licensed and non-licensed sets do follow an identical trend over the years, meaning that price increases/decreases have affected all Lego sets, not just licensed or non-licensed. It’s fair to assume that these changes are probably caused by economical factors (production costs, etc.), as this would indeed affect all sets. Price-to-part ratios started to rise in 2020 and peaked at 2021, and after that decreased and stabilized again. I have no idea about the exact cause of this bump, but a possible explanation could be the effect of COVID-19 on the economy.\n\n\n5.4 Retail price versus set sizes\nLet’s now look at the correlation between piece count and retail price for all the sets that have been released from 2018 to 2024.\n\n\nShow code\n#For all linear regressions, the intercept is set to 0, since a piece count of 0 should also result in a retail price of 0.\n#Linear regression for all sets.\nlm_all &lt;- lm(DE_retailPrice ~ pieces - 1, data = df_sets_18to24_filt)\nsummary(lm_all)\n## \n## Call:\n## lm(formula = DE_retailPrice ~ pieces - 1, data = df_sets_18to24_filt)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -735.91   -0.41    5.36   14.05  436.95 \n## \n## Coefficients:\n##         Estimate Std. Error t value Pr(&gt;|t|)    \n## pieces 0.0843011  0.0006785   124.2   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 35.29 on 2525 degrees of freedom\n## Multiple R-squared:  0.8594, Adjusted R-squared:  0.8594 \n## F-statistic: 1.544e+04 on 1 and 2525 DF,  p-value: &lt; 2.2e-16\n\n#Linear regression for non-licensed sets.\nlm_nonlic &lt;- lm(DE_retailPrice ~ pieces - 1, data = filter(df_sets_18to24_filt, License == \"No license\"))\nsummary(lm_nonlic)\n## \n## Call:\n## lm(formula = DE_retailPrice ~ pieces - 1, data = filter(df_sets_18to24_filt, \n##     License == \"No license\"))\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -583.96    2.00    6.50   15.91  427.34 \n## \n## Coefficients:\n##         Estimate Std. Error t value Pr(&gt;|t|)    \n## pieces 0.0713083  0.0008962   79.57   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 34.07 on 1308 degrees of freedom\n## Multiple R-squared:  0.8288, Adjusted R-squared:  0.8286 \n## F-statistic:  6331 on 1 and 1308 DF,  p-value: &lt; 2.2e-16\n\n#Linear regression for non-licensed sets.\nlm_lic &lt;- lm(DE_retailPrice ~ pieces - 1, data = filter(df_sets_18to24_filt, License == \"Licensed\"))\nsummary(lm_lic)\n## \n## Call:\n## lm(formula = DE_retailPrice ~ pieces - 1, data = filter(df_sets_18to24_filt, \n##     License == \"Licensed\"))\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -301.50   -3.24    3.80   12.06  394.00 \n## \n## Coefficients:\n##         Estimate Std. Error t value Pr(&gt;|t|)    \n## pieces 0.0991981  0.0008485   116.9   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 30.12 on 1216 degrees of freedom\n## Multiple R-squared:  0.9183, Adjusted R-squared:  0.9182 \n## F-statistic: 1.367e+04 on 1 and 1216 DF,  p-value: &lt; 2.2e-16\n\n#Getting the slope from the fitted models.\nslope_all &lt;- lm_all$coefficients[1]\nslope_nonlic &lt;- lm_nonlic$coefficients[1]\nslope_lic &lt;- lm_lic$coefficients[1]\n\n#Plot.\nggplot(df_sets_18to24_filt) +\n  geom_point(aes(x = pieces, y = DE_retailPrice, color = License, fill = License), pch = 21, size = 3) +\n  geom_abline(slope = slope_all, color = \"black\", lty = 1) +\n  geom_abline(slope = slope_nonlic, color = \"#9a1917\", lty = 1) +\n  geom_abline(slope = slope_lic, color = \"#d88c0f\", lty = 1) +\n  geom_text(aes(x = 8700, y = 925, label = paste0(\"slope = \", round(slope_lic, 4))), color = \"#d88c0f\", angle = 36.3, size = 5) +\n    geom_text(aes(x = 9400, y = 840, label = paste0(\"slope = \", round(slope_all, 4))), color = \"black\", angle = 32.1, size = 5) +\n      geom_text(aes(x = 11000, y = 830, label = paste0(\"slope = \", round(slope_nonlic, 4))), color = \"#9a1917\", angle = 28.2, size = 5) +\n  scale_x_continuous(name = \"Piece count\", breaks = seq(0, 12000, 2000)) +\n  scale_y_continuous(name = \"Retail price (EUR)\", breaks = seq(0, 1000, 200)) +\n  scale_color_manual(values = c(\"#9a1917\", \"#d88c0f\")) +\n  scale_fill_manual(values = c(\"#9a19174D\", \"#d88c0f4D\")) +\n  coord_cartesian(xlim = c(0, 12000), ylim = c(0, 1000)) +\n  theme_classic() +\n  theme(axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16, color = \"black\"),\n        legend.position = \"bottom\",\n        legend.title = element_blank(),\n        legend.text = element_text(size = 14))\n\n\n\n\n\n\n\n\nFigure 6: Correlation between set piece count and retail price for all sets released in 2018 to 2024. Linear regression models are made for non-licensed, licensed, and all sets.\n\n\n\n\n\nFrom Figure 5, we know already that there is indeed a price difference between licensed and non-licensed sets, so it is not a complete surprise that the fitted linear regressions differs when sub setting for licensed and non-licensed sets ( Figure 6 ). In a way, the slope of these linear regressions can be seen as an alternative way to estimate the price-to-part ratio, since multiplying the slope by the piece count results in the retail price (or more correctly: an estimation of the retail price according to the fitted linear model).\nYou can find the full details of the three fitted linear models in the code used to generate Figure 6. All three have an R2 between 0.83 and 0.92 , which shows that they are a pretty good fit for the data. From this you could conclude that the correlation between piece count and retail price is pretty linear, meaning that sets with a high piece count do not become cheaper to produce.\nTo have a look at what are the cheapest and most expensive sets within each category (licensed/non-licensed), I want to select the sets that deviate strongly from the linear regression. To have an idea for the cut-off to use for outliers, I used the standard deviation on the retail price of all sets (which is 74.5 EUR) and increased this to 100 EUR. I then used this to filter df_sets_18to24_filt for licensed sets where the retail price deviates more than 100 EUR upwards of the fitted linear regression.\n\n\nShow code\n#Filter for licensed sets that deviate 100 EUR above the fitted linear regression \ndf_lic_exp &lt;- df_sets_18to24_filt %&gt;%\n  filter(DE_retailPrice &gt; (slope_lic*pieces) + 100) %&gt;%\n  filter(License == \"Licensed\") %&gt;%\n  mutate(\"Dev_lm\" = abs((slope_lic*pieces + 100) - DE_retailPrice))\n\n#Plot.\nggplot(df_lic_exp) +\n  geom_point(aes(x = pieces, y = DE_retailPrice, color = License, fill = License), pch = 21, size = 3) +\n  geom_abline(slope = slope_lic, color = \"#d88c0f\", lty = 1) +\n  geom_abline(slope = slope_lic, intercept = 100, color = \"#d88c0f\", lty = 2) +\n    geom_text_repel(aes(x = pieces, y = DE_retailPrice, label = name), box.padding = 0.25, max.overlaps = Inf, segment.curvature = -0.1, segment.ncp = 3, segment.angle = 20, min.segment.length = 0.25) +\n  scale_x_continuous(name = \"Piece count\", breaks = seq(0, 12000, 2000)) +\n  scale_y_continuous(name = \"Retail price (EUR)\", breaks = seq(0, 1000, 200)) +\n  scale_color_manual(values = c(\"#d88c0f\")) +\n  scale_fill_manual(values = c(\"#d88c0f4D\")) +\n  coord_cartesian(xlim = c(0, 12000), ylim = c(0, 1000)) +\n  theme_classic() +\n  theme(axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16, color = \"black\"),\n        legend.position = \"bottom\",\n        legend.title = element_blank(),\n        legend.text = element_text(size = 14))\n\n\n\n\n\n\n\n\nFigure 7: Licensed sets that deviate more than 100 EUR above the linear regression of licensed sets (indicated by the dashed line).\n\n\n\n\n\nNot a lot of surprises in Figure 7, showing sets that already were known for their hefty retail price. Absolute champion is the Liebherr Crawler Crane (42146), where not only the license, but also the included motors and control app have its effect on the retail price. In second place is the Star Wars Imperial Star Destroyer (75252).\nSimilarly, I looked at the licensed sets that are on the cheap end of the fitted linear regression:\n\n\nShow code\n#Filter for licensed sets that deviate 100 EUR below the fitted linear regression.\ndf_lic_chp &lt;- df_sets_18to24_filt %&gt;%\n  filter(DE_retailPrice &lt; (slope_lic*pieces) - 100) %&gt;%\n  filter(License == \"Licensed\") %&gt;%\n  mutate(\"Dev_lm\" = abs((slope_lic*pieces - 100) - DE_retailPrice))\n\n#Plot.\nggplot(df_lic_chp) +\n  geom_point(aes(x = pieces, y = DE_retailPrice, color = License, fill = License), pch = 21, size = 3) +\n  geom_abline(slope = slope_lic, color = \"#d88c0f\", lty = 1) +\n  geom_abline(slope = slope_lic, intercept = -100, color = \"#d88c0f\", lty = 2) +\n  geom_text_repel(aes(x = pieces, y = DE_retailPrice, label = name), box.padding = 0.25, max.overlaps = Inf, segment.curvature = -0.1, segment.ncp = 3, segment.angle = 20, min.segment.length = 0.25) +\n  scale_y_continuous(name = \"Retail price (EUR)\", breaks = seq(0, 1000, 200)) +\n  scale_x_continuous(name = \"Piece count\", breaks = seq(0, 12000, 2000)) +\n  scale_color_manual(values = c(\"#d88c0f\")) +\n  scale_fill_manual(values = c(\"#d88c0f4D\")) +\n  coord_cartesian(xlim = c(0, 12000), ylim = c(0, 1000)) +\n  theme_classic() +\n  theme(axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16, color = \"black\"),\n        legend.position = \"bottom\",\n        legend.title = element_blank(),\n        legend.text = element_text(size = 14))\n\n\n\n\n\n\n\n\nFigure 8: Licensed sets that deviate more than 100 EUR below the linear regression of licensed sets (indicated by the dashed line).\n\n\n\n\n\nFigure 8 shows two contenders for the cheapest licensed set: Harry Potter Hogwarts Crests (31201), and Jim Lee Batman Collection (31205). Both sets are part of the Lego Art theme, and use 1x1 round plates and/or tiles to create what is in essence a pixel artwork. Content-wise, the vast majority of these sets consist of these tiny 1x1 round plates/tiles, which do not require a lot of plastic to produce, making them cheap. Indeed, even the next three cheapest sets in line are again in the Lego Art theme: Star Wars The Sith (31200), Marvel Studios Iron Man (31199), and Disney’s Mickey Mouse (31202). The first non-Art set is Hogwart’s Castle, a microfig scale rendition of the eponymous castle from Harry Potter.\nFollowing the same principle, I filtered for the most expensive non-licensed sets:\n\n\nShow code\n#Filter for non-licensed sets that deviate 100 EUR above the fitted linear regression.\ndf_nonlic_exp &lt;- df_sets_18to24_filt %&gt;%\n  filter(DE_retailPrice &gt; (slope_nonlic*pieces) + 100) %&gt;%\n  filter(License == \"No license\") %&gt;%\n  mutate(\"Dev_lm\" = abs((slope_nonlic*pieces + 100) - DE_retailPrice))\n\n#Plot.\nggplot(df_nonlic_exp) +\n  geom_point(aes(x = pieces, y = DE_retailPrice, color = License, fill = License), pch = 21, size = 3) +\n  geom_abline(slope = slope_nonlic, color = \"#9a1917\", lty = 1) +\n  geom_abline(slope = slope_nonlic, intercept = +100, color = \"#9a1917\", lty = 2) +\n    geom_text_repel(aes(x = pieces, y = DE_retailPrice, label = name), box.padding = 0.25, max.overlaps = Inf, segment.curvature = -0.1, segment.ncp = 3, segment.angle = 20, min.segment.length = 0.25, max.iter = 30000, force = 5) +\n  scale_x_continuous(name = \"Piece count\", breaks = seq(0, 12000, 2000)) +\n  scale_y_continuous(name = \"Retail price (EUR)\", breaks = seq(0, 1000, 200)) +\n  scale_color_manual(values = c(\"#9a1917\")) +\n  scale_fill_manual(values = c(\"#9a19174D\")) +\n  coord_cartesian(xlim = c(0, 12000), ylim = c(0, 1000)) +\n  theme_classic() +\n  theme(axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16, color = \"black\"),\n        legend.position = \"bottom\",\n        legend.title = element_blank(),\n        legend.text = element_text(size = 14))\n\n\n\n\n\n\n\n\nFigure 9: Non-licensed sets that deviate more than 100 EUR above the linear regression of non-licensed sets.\n\n\n\n\n\nThe first three most expensive sets in Figure 9 are part of the educational SPIKE theme (45678 and 45345) or Mindstorms (51515), which explains their high price. Fourth place is the 4x4 X-Treme Off-Roader Technic set, coming with motors and a control app, again explaining its higher price. Fifth is the first real model set: the Motorized Lighthouse (21335), which - as the name implies - also comes with a motor, but is first and foremost a Lego model set.\n\n\nShow code\n#Filter for non-licensed sets that deviate 100 EUR below the fitted linear regression.\ndf_nonlic_chp &lt;- df_sets_18to24_filt %&gt;%\n  filter(DE_retailPrice &lt; (slope_nonlic*pieces) - 100) %&gt;%\n  filter(License == \"No license\") %&gt;%\n  mutate(\"Dev_lm\" = abs((slope_nonlic*pieces - 100) - DE_retailPrice))\n\n#Plot.\nggplot(df_nonlic_chp) +\n  geom_point(aes(x = pieces, y = DE_retailPrice, color = License, fill = License), pch = 21, size = 3) +\n  geom_abline(slope = slope_nonlic, color = \"#9a1917\", lty = 1) +\n  geom_abline(slope = slope_nonlic, intercept = -100, color = \"#9a1917\", lty = 2) +\n  geom_text_repel(aes(x = pieces, y = DE_retailPrice, label = name), box.padding = 0.25, max.overlaps = Inf, segment.curvature = -0.1, segment.ncp = 3, segment.angle = 20, min.segment.length = 0.25, max.iter = 30000, force = 5) +\n  scale_x_continuous(name = \"Piece count\", breaks = seq(0, 12000, 2000)) +\n  scale_y_continuous(name = \"Retail price (EUR)\", breaks = seq(0, 1000, 200)) +\n  scale_color_manual(values = c(\"#9a1917\")) +\n  scale_fill_manual(values = c(\"#9a19174D\")) +\n  coord_cartesian(xlim = c(0, 12000), ylim = c(0, 1000)) +\n  theme_classic() +\n  theme(axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16, color = \"black\"),\n        legend.position = \"bottom\",\n        legend.title = element_blank(),\n        legend.text = element_text(size = 14))\n\n\n\n\n\n\n\n\nFigure 10: Non-licensed sets that deviate more than 100 EUR below the linear regression of non-licensed sets.\n\n\n\n\n\nFigure 10 is an easy one: every single set on here is an Art theme mosaic set consisting mostly of 1x1 plates and tiles. As was mentioned in Figure 8, these parts are cheap to produce, and make up most of the sets, explaining their cheaper price range."
  },
  {
    "objectID": "Personal_projects/posts/LegoSetPrices/LegoSetPrices.html#summary",
    "href": "Personal_projects/posts/LegoSetPrices/LegoSetPrices.html#summary",
    "title": "A closer look at Lego set prices",
    "section": "6 Summary",
    "text": "6 Summary\nFrom this quick exploration of the Lego dataset, we can extract the following conclusions:\n\nLego overall retail prices have increased between 2018 and 2024, but so have the set sizes. Lego seems to release progresively bigger sets (Figure 2), possibly because they increasingly target an older audience that prefers more detailed and intricate models. As a logical consequence, the overall retail prices increase as well (Figure 3).\nPrice-to-part ratios are consistently higher for licensed sets. As expected, licensed sets have an increased price-to-part ratio, with a difference ranging from 1.1 - 1.6 cents per piece (Figure 5). Both licensed and non-licensed sets follow the same trends from 20218 to 2024, indicating that external (economic) factors influence the price of all sets equally.\nSet piece count and retail price follow a linear correlation. Set retail price is independent of the set size (except for some sets with highly specialized parts, see Table 3). Hence, bigger sets are not cheaper to produce than smaller ones.\nThe Lego Art theme has the cheapest price-to-part ratio. It does not matter if they’re licensed (Figure 8) or not (Figure 10), several of the Art theme sets end up being the cheapest in both categories. Possible explanation is that the sets are mainly 1x1 round plates and tiles.\nLiebherr Crawler Crane LR 13000 (42146) and 4x4 X-Treme Off-Roader (42099) are the most expensive sets in the licensed and non-licensed category. In the non-licensed category, the 4x4 X-Treme Off-Roader is preceded by three SPIKE and Mindstorm sets (Figure 9), but these can be considered highly specialized and don’t really fall into the Lego model set category.\nSecond places go to the Star Wars Imperial Star Destroyer (75252), and the Motorized Lighthouse (21335).\n\nAll-in-all it is interesting to see that the price-to-part ratio did increase a during what was probably a COVID-induced (?) economical situation (Figure 5). As of 2024, it looks like this has stabilized to the pre-2020 situation. With Lego releasing increasingly bigger sets (Figure 2), this might give the impression that Lego sets overall are getting more and more expensive, but as we see from the price-to-part ratio, the opposite is actually true (at least for the years 2018-2024)."
  }
]